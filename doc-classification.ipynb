{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport shutil\nimport os\nimport tarfile\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pickle\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nimport pandas as pd\nfrom multiprocessing import Process# this is used for multithreading\nimport multiprocessing\nimport codecs# this is used for file operations \nimport random as r\nimport time\nimport math\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import log_loss,confusion_matrix\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,MaxPooling2D,Conv2D,Dropout,BatchNormalization\nfrom keras import backend as k \nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n# from keras.utils import np_utils\nfrom keras.preprocessing import image\nfrom sklearn.datasets import load_files\nimport cv2\nimport pickle\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,precision_score,recall_score\nfrom statistics import mode\nfrom sklearn.utils.multiclass import unique_labels","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:52:26.298949Z","iopub.execute_input":"2023-11-12T14:52:26.299554Z","iopub.status.idle":"2023-11-12T14:52:38.026689Z","shell.execute_reply.started":"2023-11-12T14:52:26.299522Z","shell.execute_reply":"2023-11-12T14:52:38.025875Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"label_path='/kaggle/input/labels-data/labels/'\n# This function will create an index file with image paths and labels in csv format\n# obtained csv will have two columns path(path to acess the image),label(class of the image)\ndef create_csv(file_path):\n    images=[]\n    labels=[]\n    label_file=pd.DataFrame(columns=['path','label'],index=None)\n    file=open(file_path)\n    for ele in file:\n        images.append(ele.split(' ')[0])\n        labels.append(ele.split(' ')[1].rstrip())\n    label_file.path=images\n    label_file.label=labels\n    return label_file\n\ntrain_label_path=label_path+'train.txt' \ntest_label_path=label_path+'test.txt' \nval_label_path=label_path+'val.txt'\n# creating index file for train,test and val dataset\ntrain_label=create_csv(train_label_path)\ntest_label=create_csv(test_label_path)\nval_label=create_csv(val_label_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:52:38.028628Z","iopub.execute_input":"2023-11-12T14:52:38.029350Z","iopub.status.idle":"2023-11-12T14:52:38.590167Z","shell.execute_reply.started":"2023-11-12T14:52:38.029312Z","shell.execute_reply":"2023-11-12T14:52:38.589151Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"val_label","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:24:07.025976Z","iopub.execute_input":"2023-11-12T14:24:07.026396Z","iopub.status.idle":"2023-11-12T14:24:07.064145Z","shell.execute_reply.started":"2023-11-12T14:24:07.026347Z","shell.execute_reply":"2023-11-12T14:24:07.062178Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             path label\n0           imagesg/g/t/h/gth35e00/2024525661.tif    11\n1      imagesi/i/y/k/iyk38c00/512015827+-5827.tif     0\n2             imagesr/r/r/e/rre21e00/87103403.tif     0\n3             imagesk/k/s/u/ksu44c00/03636607.tif     4\n4        imagesr/r/a/i/rai09d00/50437856-7857.tif    14\n...                                           ...   ...\n39995       imageso/o/u/k/ouk93f00/0013006838.tif    10\n39996       imagesf/f/f/b/ffb52c00/2074103881.tif    11\n39997       imagesg/g/h/b/ghb11f00/0001251052.tif    15\n39998       imagesl/l/c/k/lck71f00/2016003416.tif     9\n39999       imagesa/a/y/g/ayg48e00/2023114503.tif    15\n\n[40000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>imagesg/g/t/h/gth35e00/2024525661.tif</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>imagesi/i/y/k/iyk38c00/512015827+-5827.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>imagesr/r/r/e/rre21e00/87103403.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>imagesk/k/s/u/ksu44c00/03636607.tif</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>imagesr/r/a/i/rai09d00/50437856-7857.tif</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>imageso/o/u/k/ouk93f00/0013006838.tif</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>imagesf/f/f/b/ffb52c00/2074103881.tif</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>imagesg/g/h/b/ghb11f00/0001251052.tif</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>imagesl/l/c/k/lck71f00/2016003416.tif</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>imagesa/a/y/g/ayg48e00/2023114503.tif</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# denifing the metadatas\ntrain_dir='train'\ntest_dir='test'\ncv_dir='val'\nclasses = {'0': 'letter',\n '1': 'form',\n '10': 'budget',\n '11': 'invoice',\n '12': 'presentation',\n '13': 'questionnaire',\n '14': 'resume',\n '15': 'memo',\n '2': 'email',\n '3': 'handwritten',\n '4': 'advertisement',\n '5': 'scientific report',\n '6': 'scientific publication',\n '7': 'specification',\n '8': 'file folder',\n '9': 'news article'}","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:52:38.591538Z","iopub.execute_input":"2023-11-12T14:52:38.592275Z","iopub.status.idle":"2023-11-12T14:52:38.598209Z","shell.execute_reply.started":"2023-11-12T14:52:38.592240Z","shell.execute_reply":"2023-11-12T14:52:38.597323Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Creating train,validation and Test directories\n# os.mkdir('train')\nos.mkdir('test')\nos.mkdir('val')","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:53:18.447530Z","iopub.execute_input":"2023-11-12T14:53:18.447897Z","iopub.status.idle":"2023-11-12T14:53:18.452829Z","shell.execute_reply.started":"2023-11-12T14:53:18.447869Z","shell.execute_reply":"2023-11-12T14:53:18.451842Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# run this cell to enable intra domain transfer learning\nregions = ['Whole','header', 'footer', 'left_body', 'right_body']\nfor region in regions:\n#     os.mkdir('train/'+region)\n    os.mkdir('val/'+region)\nclasses_name=[v for (k,v) in classes.items()]\nfor region in regions:\n    for label in classes_name:\n#         os.mkdir('train/'+region+'/'+label)\n        os.mkdir('val/'+region+'/'+label)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:53:20.809114Z","iopub.execute_input":"2023-11-12T14:53:20.810173Z","iopub.status.idle":"2023-11-12T14:53:20.818597Z","shell.execute_reply.started":"2023-11-12T14:53:20.810129Z","shell.execute_reply":"2023-11-12T14:53:20.817715Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/train', ignore_errors=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:29:17.223750Z","iopub.execute_input":"2023-11-12T14:29:17.225214Z","iopub.status.idle":"2023-11-12T14:29:17.231061Z","shell.execute_reply.started":"2023-11-12T14:29:17.225168Z","shell.execute_reply":"2023-11-12T14:29:17.229952Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport shutil\nimport pandas as pd\n\n# Assuming 'label' column in train_label contains the label information\n# classes is a dictionary that maps label to the destination folder\n# For example, classes = {'label1': 'folder1', 'label2': 'folder2', ...}\n\n# Create a new DataFrame to store the count of images for each label\nlabel_counts = train_label['label'].value_counts()\n\n# Set the maximum number of images per label\nmax_images_per_label = 1000\n\nfor label, count in tqdm(label_counts.items()):\n    # Limit the number of images to move for each label\n    num_images_to_move = min(count, max_images_per_label)\n\n    # Filter the DataFrame for the current label and select the first `num_images_to_move` images\n    images_to_move = train_label[train_label['label'] == label].head(num_images_to_move)\n\n    for i in range(len(images_to_move)):\n        source = '/kaggle/input/rvlcdip/images/' + images_to_move.iloc[i]['path']\n        dest = 'train/Whole/' + classes[str(label)]\n\n        if not os.path.exists(dest):\n            os.makedirs(dest)\n\n        dest_file = os.path.join(dest, images_to_move.iloc[i]['path'].split('/')[-1])\n\n        if not os.path.exists(dest_file):\n            shutil.copy(source, dest)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T14:53:24.710594Z","iopub.execute_input":"2023-11-12T14:53:24.710940Z","iopub.status.idle":"2023-11-12T14:55:22.932048Z","shell.execute_reply.started":"2023-11-12T14:53:24.710915Z","shell.execute_reply":"2023-11-12T14:55:22.931134Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"16it [01:58,  7.39s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport shutil\nimport pandas as pd\n\n# Assuming 'label' column in train_label contains the label information\n# classes is a dictionary that maps label to the destination folder\n# For example, classes = {'label1': 'folder1', 'label2': 'folder2', ...}\n\n# Create a new DataFrame to store the count of images for each label\nlabel_counts = val_label['label'].value_counts()\n\n# Set the maximum number of images per label\nmax_images_per_label = 200\n\nfor label, count in tqdm(label_counts.items()):\n    # Limit the number of images to move for each label\n    num_images_to_move = min(count, max_images_per_label)\n\n    # Filter the DataFrame for the current label and select the first `num_images_to_move` images\n    images_to_move = val_label[val_label['label'] == label].head(num_images_to_move)\n\n    for i in range(len(images_to_move)):\n        source = '/kaggle/input/rvlcdip/images/' + images_to_move.iloc[i]['path']\n        dest = 'val/Whole/' + classes[str(label)]\n\n        if not os.path.exists(dest):\n            os.makedirs(dest)\n\n        dest_file = os.path.join(dest, images_to_move.iloc[i]['path'].split('/')[-1])\n\n        if not os.path.exists(dest_file):\n            shutil.copy(source, dest)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:07:46.178631Z","iopub.execute_input":"2023-11-12T15:07:46.179641Z","iopub.status.idle":"2023-11-12T15:07:46.651299Z","shell.execute_reply.started":"2023-11-12T15:07:46.179605Z","shell.execute_reply":"2023-11-12T15:07:46.650347Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"16it [00:00, 35.23it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"val_dir='val/Whole'\ncount,limit=0,2000\nfor folder in tqdm(os.listdir(val_dir)):\n  #os.shuffle(val_dir+'/'+folder)\n  for image in os.listdir(val_dir+'/'+folder):\n        img = cv2.imread(val_dir+'/'+folder+'/'+image,0)\n        height = img.shape[0]\n        width = img.shape[1]\n        header = img[0:(int(height*0.33)), 0:width]\n        footer = img[int(height*0.67):height, 0:width]\n        left_body = img[int(height*0.33):int(height*0.67), 0:int(width*0.5)]\n        right_body = img[int(height*0.33):int(height*0.67), int(width*0.5):width]\n        header_path='val/header/'+folder+'/'+image.split('/')[-1]\n        footer_path='val/footer/'+folder+'/'+image.split('/')[-1]\n        left_path='val/left_body/'+folder+'/'+image.split('/')[-1]\n        right_path='val/right_body/'+folder+'/'+image.split('/')[-1]\n        if(os.path.exists(header_path)==False):\n            cv2.imwrite(header_path, header)\n        if(os.path.exists(footer_path)==False):\n            cv2.imwrite(footer_path, footer)\n        if(os.path.exists(left_path)==False):\n            cv2.imwrite(left_path, left_body)\n        if(os.path.exists(right_path)==False):\n            cv2.imwrite(right_path, right_body)\n        count+=1\n        if(count==limit):\n            break","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:07:50.537936Z","iopub.execute_input":"2023-11-12T15:07:50.538882Z","iopub.status.idle":"2023-11-12T15:08:15.637143Z","shell.execute_reply.started":"2023-11-12T15:07:50.538843Z","shell.execute_reply":"2023-11-12T15:08:15.636230Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 16/16 [00:25<00:00,  1.57s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training configuaration for CNN- to be trained on whole images\nimg_width, img_height = 224,224\ntrain_dir = \"train/Whole\"\nval_dir = \"val/Whole\"\ntest_dir=\"test\"\nnb_train_samples = 16000\nnb_validation_samples = 3200 \nbatch_size = 32\nepochs = 10\nnum_classes=16\nlog=[]","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:11:13.712796Z","iopub.execute_input":"2023-11-12T15:11:13.713419Z","iopub.status.idle":"2023-11-12T15:11:13.718446Z","shell.execute_reply.started":"2023-11-12T15:11:13.713389Z","shell.execute_reply":"2023-11-12T15:11:13.717420Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# preparing Training and Validation data using ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\nrescale = 1./255,\nfeaturewise_std_normalization=True)\n\nval_datagen = ImageDataGenerator(\nrescale = 1./255,\nfeaturewise_std_normalization=True)\n\ntrain_generator = train_datagen.flow_from_directory(\ntrain_dir,\ntarget_size = (img_height, img_width),\nbatch_size = batch_size, \nclass_mode = \"categorical\")\n\nval_generator = val_datagen.flow_from_directory(\nval_dir,\ntarget_size = (img_height, img_width),\nbatch_size = batch_size, \nclass_mode = \"categorical\")","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:11:19.304714Z","iopub.execute_input":"2023-11-12T15:11:19.305513Z","iopub.status.idle":"2023-11-12T15:11:20.479440Z","shell.execute_reply.started":"2023-11-12T15:11:19.305480Z","shell.execute_reply":"2023-11-12T15:11:20.478667Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Found 16000 images belonging to 16 classes.\nFound 3200 images belonging to 16 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing keras InceptionRsnetv2 pretrained model (on ImageNet)\n# We will use the ImageNet weight to initialize the model and fine tune using backpropagation(Transfer Learning) \nfrom keras import regularizers\nmodel=applications.InceptionResNetV2(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\nfor layer in model.layers[:15]:\n    layer.trainable = False\n#Adding custom Layers \nx = model.output\nx=Dropout(0.5)(x)\nx = Flatten()(x)\noutput = Dense(num_classes, activation=\"softmax\")(x)\nmodel = Model(inputs = model.input, outputs = output)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:36:01.829534Z","iopub.execute_input":"2023-11-12T15:36:01.829892Z","iopub.status.idle":"2023-11-12T15:36:07.754899Z","shell.execute_reply.started":"2023-11-12T15:36:01.829867Z","shell.execute_reply":"2023-11-12T15:36:07.754119Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n\ncheckpoint = ModelCheckpoint(\"inceptionresnet.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max', period=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.01, mode='max')\ncallbacks = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=10,\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    callbacks=callbacks,\n    verbose=1\n)\n\nlog.append(history)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T15:36:11.905000Z","iopub.execute_input":"2023-11-12T15:36:11.905815Z","iopub.status.idle":"2023-11-12T16:05:58.970343Z","shell.execute_reply.started":"2023-11-12T15:36:11.905783Z","shell.execute_reply":"2023-11-12T16:05:58.969431Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"2023-11-12 15:36:43.880122: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"500/500 [==============================] - ETA: 0s - loss: 2.2941 - accuracy: 0.4957\nEpoch 1: val_accuracy improved from -inf to 0.40000, saving model to inceptionresnet.h5\n500/500 [==============================] - 256s 346ms/step - loss: 2.2941 - accuracy: 0.4957 - val_loss: 6575.1992 - val_accuracy: 0.4000 - lr: 0.0010\nEpoch 2/10\n500/500 [==============================] - ETA: 0s - loss: 2.1780 - accuracy: 0.5815\nEpoch 2: val_accuracy improved from 0.40000 to 0.59000, saving model to inceptionresnet.h5\n500/500 [==============================] - 170s 340ms/step - loss: 2.1780 - accuracy: 0.5815 - val_loss: 6.3018 - val_accuracy: 0.5900 - lr: 0.0010\nEpoch 3/10\n500/500 [==============================] - ETA: 0s - loss: 2.3873 - accuracy: 0.6101\nEpoch 3: val_accuracy did not improve from 0.59000\n500/500 [==============================] - 169s 338ms/step - loss: 2.3873 - accuracy: 0.6101 - val_loss: 454.0276 - val_accuracy: 0.4997 - lr: 0.0010\nEpoch 4/10\n500/500 [==============================] - ETA: 0s - loss: 1.6309 - accuracy: 0.6662\nEpoch 4: val_accuracy improved from 0.59000 to 0.66438, saving model to inceptionresnet.h5\n500/500 [==============================] - 170s 340ms/step - loss: 1.6309 - accuracy: 0.6662 - val_loss: 15.6355 - val_accuracy: 0.6644 - lr: 0.0010\nEpoch 5/10\n500/500 [==============================] - ETA: 0s - loss: 1.3943 - accuracy: 0.6921\nEpoch 5: val_accuracy improved from 0.66438 to 0.68437, saving model to inceptionresnet.h5\n500/500 [==============================] - 170s 340ms/step - loss: 1.3943 - accuracy: 0.6921 - val_loss: 1.4401 - val_accuracy: 0.6844 - lr: 0.0010\nEpoch 6/10\n500/500 [==============================] - ETA: 0s - loss: 1.0083 - accuracy: 0.7361\nEpoch 6: val_accuracy improved from 0.68437 to 0.69719, saving model to inceptionresnet.h5\n500/500 [==============================] - 170s 340ms/step - loss: 1.0083 - accuracy: 0.7361 - val_loss: 1.0367 - val_accuracy: 0.6972 - lr: 0.0010\nEpoch 7/10\n500/500 [==============================] - ETA: 0s - loss: 0.8022 - accuracy: 0.7734\nEpoch 7: val_accuracy improved from 0.69719 to 0.72344, saving model to inceptionresnet.h5\n500/500 [==============================] - 174s 348ms/step - loss: 0.8022 - accuracy: 0.7734 - val_loss: 0.9717 - val_accuracy: 0.7234 - lr: 0.0010\nEpoch 8/10\n500/500 [==============================] - ETA: 0s - loss: 0.6614 - accuracy: 0.8084\nEpoch 8: val_accuracy did not improve from 0.72344\n500/500 [==============================] - 169s 337ms/step - loss: 0.6614 - accuracy: 0.8084 - val_loss: 1.9812 - val_accuracy: 0.7194 - lr: 0.0010\nEpoch 9/10\n500/500 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.8322\nEpoch 9: val_accuracy did not improve from 0.72344\n500/500 [==============================] - 169s 338ms/step - loss: 0.5911 - accuracy: 0.8322 - val_loss: 72.1762 - val_accuracy: 0.6862 - lr: 0.0010\nEpoch 10/10\n500/500 [==============================] - ETA: 0s - loss: 0.4763 - accuracy: 0.8609\nEpoch 10: val_accuracy did not improve from 0.72344\n500/500 [==============================] - 169s 337ms/step - loss: 0.4763 - accuracy: 0.8609 - val_loss: 1.0959 - val_accuracy: 0.7197 - lr: 0.0010\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=10,\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    callbacks=callbacks,\n    verbose=1\n)\n\nlog.append(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions\nfrom keras.models import load_model\nfrom keras.preprocessing import image\n\nimg = image.load_img('/kaggle/input/test-image/50643700-3700.tif', target_size=(224, 224))\nimg_array = image.img_to_array(img)\nimg_array = img_array.reshape((1, 224, 224, 3))\nimg_array /= 255.0\n\nprediction = model.predict(img_array)\npredicted_class_index = int(np.argmax(prediction, axis=-1))\npredicted_class_name = classes.get(str(predicted_class_index), 'unknown')\nprint(\"Predicted Folder: \", predicted_class_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:39:21.395226Z","iopub.execute_input":"2023-11-12T16:39:21.395601Z","iopub.status.idle":"2023-11-12T16:39:21.510462Z","shell.execute_reply.started":"2023-11-12T16:39:21.395572Z","shell.execute_reply":"2023-11-12T16:39:21.509467Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 36ms/step\nPredicted Folder:  presentation\n","output_type":"stream"}]}]}